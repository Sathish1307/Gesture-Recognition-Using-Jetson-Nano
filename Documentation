1. Overview
The rapid advancement of embedded systems and edge artificial intelligence has enabled the development of intelligent, real-time human–machine interaction systems. 
This project presents the design and implementation of a real-time hand gesture recognition system deployed on the NVIDIA Jetson Nano platform.
The system enables touchless media control using predefined hand gestures, eliminating the need for physical interaction with conventional input devices such as keyboards, remotes, or touchscreens.
The proposed solution integrates computer vision techniques with AI-based hand landmark detection to recognize specific gestures and translate them into system-level media commands. 
By performing inference directly on the Jetson Nano, the system ensures low latency, improved responsiveness, and independence from cloud-based processing. 
This makes the solution suitable for embedded edge applications where real-time performance and computational efficiency are essential.

1.1 Problem Statement
In modern interactive systems, traditional input methods often require direct physical contact, which may not be suitable in environments demanding hygiene, safety, or convenience.
Applications such as smart homes, healthcare facilities, industrial automation systems, and embedded AI platforms increasingly require contactless and intuitive interaction mechanisms.
The primary challenge addressed in this project is the development of a lightweight, accurate, and real-time gesture recognition system capable of operating efficiently on a resource-constrained embedded device like the NVIDIA Jetson Nano. 
The system must reliably detect hand movements, classify predefined gestures under varying lighting and orientation conditions, and execute corresponding media control actions with minimal latency. Additionally, the solution must maintain stable performance without excessive computational overhead.

1.2 Objectives
The main objective of this project is to design and implement a real-time hand gesture recognition system for touchless media control using the NVIDIA Jetson Nano platform.
The system aims to demonstrate efficient edge AI deployment by integrating computer vision and embedded processing.
The specific objectives of the project include detecting hand landmarks using an optimized AI-based model, classifying gestures such as Palm and Fist through geometric analysis of landmark positions, mapping recognized gestures to operating system media control commands, and ensuring stable performance through optimized processing techniques. 
Furthermore, the system is intended to operate with low latency and high responsiveness, making it suitable for real-time embedded applications.

2. Hardware and Software Components

2.1.1 NVIDIA Jetson Nano
The NVIDIA Jetson Nano serves as the core processing unit of the system. It is a compact embedded AI development board designed specifically for edge computing and real-time vision applications.

Why It Was Chosen
The Jetson Nano was selected because it provides:

On-device GPU acceleration for AI inference

Support for computer vision frameworks

Low power consumption suitable for embedded deployment

Compatibility with Ubuntu-based operating systems

Efficient handling of real-time video processing

For this project, the Jetson Nano perfectly matches the requirements of running MediaPipe and OpenCV in real time while maintaining low latency. 
Its ability to perform AI-based inference locally eliminates dependency on cloud processing, making it ideal for edge AI applications such as gesture recognition.

2.1.2 USB / Camera Module
A camera module is used to capture real-time video input of the user’s hand gestures.

Why It Was Chosen
The camera is essential for acquiring continuous image frames that serve as input to the gesture recognition pipeline. It provides:

Real-time video feed

Sufficient resolution for accurate hand landmark detection

Compatibility with OpenCV for seamless frame capture

The camera integrates directly with the Jetson Nano and enables consistent frame acquisition required for stable gesture detection.


2.1.3 MicroSD Card with JetPack OS
The MicroSD card contains the JetPack operating system, which is required to run the Jetson Nano.

Why It Was Chosen
JetPack provides:

Ubuntu-based environment

CUDA support

Preconfigured drivers

AI framework compatibility

It ensures that the Jetson Nano can support computer vision libraries and AI models required for the project.


2.2 Software Components
2.2.1 Ubuntu (JetPack Operating System)
The system runs on Ubuntu provided through NVIDIA JetPack.

Why It Was Chosen
Ubuntu provides:

Stable Linux-based development environment

Python support

Library compatibility for OpenCV and MediaPipe

Reliable hardware driver integration

This operating system ensures smooth execution of the entire gesture recognition pipeline.

2.2.2 Python Programming Language
Python is used as the primary programming language for implementing the system.

Why It Was Chosen
Python was selected because:

It provides extensive support for computer vision libraries

It has simple and readable syntax

It integrates seamlessly with OpenCV and MediaPipe

It enables rapid development and debugging

Python allows efficient implementation of real-time gesture classification logic while maintaining code clarity.

2.2.3 OpenCV
OpenCV is used for video frame capture and image processing.

Role in the Project
Captures real-time frames from the camera

Performs color space conversion

Displays processed output

Handles video stream management

Why It Was Chosen
OpenCV is lightweight, highly optimized, and well-supported on embedded Linux platforms. It integrates directly with the Jetson Nano and provides efficient frame-level processing required for real-time performance.

2.2.4 MediaPipe
MediaPipe is used for hand landmark detection.

Role in the Project
Detects human hand in real time

Extracts 21 landmark points

Provides x, y, z coordinates for each landmark

Why It Was Chosen
MediaPipe was selected because:

It provides accurate real-time hand tracking

It is computationally efficient

It does not require custom model training

It works effectively on embedded systems

Its lightweight AI model makes it perfectly suited for running on the Jetson Nano without heavy computational overhead.

2.2.5 NumPy
NumPy is used for numerical computations and coordinate comparisons.

Why It Was Chosen
Gesture classification requires comparison of landmark coordinates. NumPy enables:

Efficient numerical operations

Fast array processing

Simplified mathematical logic implementation

It supports the geometric-based gesture classification logic implemented in the system.

2.2.6 pynput
The pynput library is used to simulate media key presses.

Role in the Project
Maps detected gestures to system-level media control commands

Triggers Play and Pause actions

Why It Was Chosen
pynput allows programmatic control of keyboard inputs at the operating system level. This makes it possible to connect gesture recognition output directly to media control functionality, completing the interaction loop.

Software Modules Used and Their Justification :

1. The software architecture of this project is designed to ensure efficient real-time hand gesture recognition on an embedded edge platform.
Each software component was selected based on compatibility with the NVIDIA Jetson Nano, computational efficiency, and seamless integration with computer vision and AI frameworks.

2. Ubuntu (JetPack Operating System) serves as the foundational environment for the system. It provides hardware driver support, CUDA compatibility, and a stable Linux-based platform required for running AI and vision libraries.
Its integration with the Jetson Nano ensures optimized performance and smooth hardware–software interaction.

3. Python was chosen as the primary programming language due to its simplicity, readability, and extensive ecosystem for AI and computer vision development. 
It enables rapid implementation, debugging, and integration of various libraries used in the project.

4. OpenCV is used for real-time video capture and frame processing. It manages camera input, image conversion, and display handling efficiently.
Its optimized processing capabilities ensure minimal latency during continuous video streaming, which is essential for real-time gesture recognition.

5. MediaPipe is utilized for accurate hand landmark detection. It provides 21 hand keypoints without requiring custom model training. 
Its lightweight and optimized architecture makes it highly suitable for embedded deployment, allowing the system to perform real-time inference without excessive computational load.

6. NumPy supports numerical computations required for comparing landmark coordinates during gesture classification.
It enables efficient array operations and mathematical processing, which simplifies the implementation of gesture detection logic.

7. pynput is used to simulate system-level keyboard inputs. 
It connects the gesture recognition output to media control actions such as play and pause, thereby completing the interaction loop between user gestures and system response.

Collectively, these software components form a cohesive and optimized stack that ensures real-time performance, computational efficiency, and reliable gesture-based media control on the Jetson Nano platform.

Real-World Relevance and Applications of the Project,

1. Practical Real-World Relevance
This project demonstrates a real-time hand gesture recognition system deployed on an embedded edge AI device (Jetson Nano).
In real-world environments, touchless interaction systems are becoming increasingly important due to hygiene, convenience, and accessibility requirements. 
By enabling users to control media functions using hand gestures, the system eliminates the need for physical contact with devices such as keyboards, remotes, or touchscreens.
The use of on-device AI processing ensures low latency, improved privacy, and independence from cloud services. 
This makes the system reliable for environments where internet connectivity is limited or where data security is a priority. 
The compact embedded setup further allows deployment in portable and standalone applications.

2. Consumer Electronics Applications
One of the primary applications of this project is in smart home and entertainment systems. 
Users can control music players, video playback, and presentation slides through simple hand gestures. This enhances user experience by making interaction more intuitive and seamless.

The system can also be integrated into smart TVs, kiosks, and interactive displays where remote-free control is desirable.
Gesture-based interaction reduces hardware dependency on multiple input devices and creates a modern, futuristic interface experience.

3. Industrial and Automation Applications
In industrial environments, workers often wear gloves or operate in conditions where touching control panels is inconvenient or unsafe. 
A gesture-based control system can provide a safe and efficient alternative for machine operation, monitoring systems, or robotic control interfaces.
Additionally, in laboratories or medical environments, touchless control minimizes contamination risks. The embedded nature of the system ensures stable performance without requiring heavy computing infrastructure.

Limitations of the Current System, 

a) Although the developed gesture recognition system performs effectively for predefined gestures, it has certain practical limitations that restrict its scalability and robustness in complex real-world environments.

b) One major limitation is its dependence on controlled environmental conditions. The system’s accuracy can decrease under poor lighting, extreme brightness, or background clutter. Since gesture classification is based on landmark geometry, significant hand rotation or unusual angles may lead to misclassification. Additionally, the system currently supports only a limited number of predefined gestures (Palm and Fist), which restricts its functionality to basic media control operations.

c) Lighting Sensitivity: Performance may reduce in low-light or high-glare environments.

d) Limited Gesture Set: Only predefined gestures are recognized; dynamic or complex gestures are not supported.

e) Angle and Orientation Dependency: Extreme hand tilts may affect classification accuracy.

f) Single-Hand Focus: The system primarily detects and processes one hand at a time.

g) Hardware Constraints: Being deployed on an embedded device, computational resources are limited compared to high-end systems.

Furthermore, the system relies on geometric rule-based classification rather than a trained deep learning model for gesture recognition. 
While this ensures lightweight execution, it limits adaptability and scalability for recognizing a wider range of gestures. 
Future improvements could involve advanced gesture modeling, adaptive thresholding, and multi-gesture support to enhance robustness and expand real-world applicability.


Conclusion
This project successfully demonstrates a real-time hand gesture recognition system implemented on the Jetson Nano platform. 
By integrating computer vision and embedded AI processing, the system is capable of detecting predefined gestures and converting them into media control actions.
The use of lightweight tools such as MediaPipe and OpenCV ensures smooth performance while maintaining computational efficiency on an edge device.

Overall, the project proves that reliable and responsive gesture-based control can be achieved using an embedded system without cloud dependency.
It highlights the practical application of edge AI in creating touchless human–machine interaction systems.
With further improvements and scalability enhancements, this system can be extended to support more advanced real-world applications.






